from pathlib import Path
import json
from typing import List, Dict

import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

# ===== è·¯å¾„é…ç½® / Pfad-Konfiguration =====
CHUNK_INPUT_PATH = r"D:\KI-Agent\out\besser Chunk.jsonl"
EMB_OUTPUT_PATH = r"D:\KI-Agent\out\MM_SPO13_embeddings.npy"
MAPPING_OUTPUT_PATH = r"D:\KI-Agent\out\MM_SPO13_mapping.jsonl"
FAISS_INDEX_PATH = r"D:\KI-Agent\out\MM_SPO13_faiss.index"

MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# è¿™ä¸ªæ¨¡åž‹æ”¯æŒå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬å¾·è¯­ï¼‰ï¼Œé€Ÿåº¦å’Œæ•ˆæžœéƒ½æ¯”è¾ƒå¹³è¡¡ã€‚
# Dieses Modell unterstÃ¼tzt mehrere Sprachen (inkl. Deutsch) und ist relativ schnell.


def load_chunks(path: str) -> List[Dict]:
    """
    ä»Ž JSONL æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰ chunksï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰ã€‚
    Liest alle Chunks aus einer JSONL-Datei (eine Zeile = ein JSON-Objekt).
    """
    chunks: List[Dict] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            chunks.append(json.loads(line))
    return chunks

# è¿™ä¸ªå‡½æ•°è´Ÿè´£è¯»å–ä½ å·²ç»â€œä¿®å¥½çš„â€ besser Chunk æ–‡ä»¶ï¼Œè¿”å›žä¸€ä¸ªå­—å…¸åˆ—è¡¨ã€‚
# Diese Funktion lÃ¤dt die bereinigte Chunk-Datei und gibt eine Liste von Dicts zurÃ¼ck.


def encode_chunks(chunks: List[Dict], model_name: str) -> np.ndarray:
    """
    ä½¿ç”¨ SentenceTransformer æ¨¡åž‹ï¼Œå¯¹æ¯ä¸ª chunk["text"] è®¡ç®—å‘é‡ã€‚
    Nutzt ein SentenceTransformer-Modell, um fÃ¼r jedes chunk["text"]
    einen Embedding-Vektor zu berechnen.
    """
    model = SentenceTransformer(model_name)
    texts = [ch.get("text", "") for ch in chunks]

    print(f"ðŸ§  Embeddings werden berechnet... (Anzahl Texte: {len(texts)})")
    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True,  # å½’ä¸€åŒ–ï¼Œæ–¹ä¾¿ç”¨ Inner Productâ‰ˆCosine
    )
    return embeddings

# è¿™ä¸ªå‡½æ•°ä¼šåŠ è½½é¢„è®­ç»ƒæ¨¡åž‹ï¼ŒæŠŠæ‰€æœ‰ chunk çš„ text è½¬æˆä¸€ä¸ª NÃ—D çš„å‘é‡çŸ©é˜µã€‚
# Die Funktion lÃ¤dt ein vortrainiertes Modell und wandelt alle Texte der Chunks
# in eine NÃ—D-Embedding-Matrix um.


def save_embeddings(embeddings: np.ndarray, path: str):
    """
    ä¿å­˜ embedding çŸ©é˜µä¸º .npy æ–‡ä»¶ã€‚
    Speichert die Embedding-Matrix als .npy-Datei.
    """
    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    np.save(out_path, embeddings)

# æŠŠæ‰€æœ‰å‘é‡å­˜åˆ°ç£ç›˜ï¼ŒåŽé¢å»ºç´¢å¼•æˆ–è°ƒè¯•éƒ½å¯ä»¥ç›´æŽ¥è½½å…¥è¿™ä¸ª .npyã€‚
# Speichert die Embeddings auf der Platte, damit sie spÃ¤ter fÃ¼r den Index
# oder Debugging wieder geladen werden kÃ¶nnen.


def save_mapping(chunks: List[Dict], path: str):
    """
    ä¿å­˜ index â†’ chunk ä¿¡æ¯çš„æ˜ å°„è¡¨ä¸º JSONLï¼š
    æ¯ä¸€è¡ŒåŒ…å«ï¼šidx, id, page, section, docã€‚

    Speichert die Zuordnung index â†’ Chunk-Infos als JSONL:
    jede Zeile enthÃ¤lt: idx, id, page, section, doc.
    """
    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("w", encoding="utf-8") as f:
        for idx, ch in enumerate(chunks):
            rec = {
                "idx": idx,
                "id": ch.get("id"),
                "page": ch.get("page"),
                "section": ch.get("section"),
                "doc": ch.get("doc"),
            }
            f.write(json.dumps(rec, ensure_ascii=False))
            f.write("\n")

# è¿™ä¸ªå‡½æ•°ä¼šä¸ºæ¯ä¸€è¡Œå‘é‡è®°å½•å®ƒå¯¹åº”å“ªä¸ª chunkï¼ˆid / page / section / docï¼‰ï¼Œ
# ä»¥åŽæ£€ç´¢æ—¶é€šè¿‡ idx å°±èƒ½æ‰¾åˆ°åŽŸå§‹æ–‡æœ¬ã€‚
# Diese Funktion legt fÃ¼r jeden Embedding-Vektor fest, zu welchem Chunk er gehÃ¶rt,
# sodass man nach der FAISS-Suche Ã¼ber idx wieder auf den Text zugreifen kann.


def build_faiss_index(embeddings: np.ndarray, path: str):
    """
    ä½¿ç”¨ FAISS å»ºç«‹å‘é‡ç´¢å¼•ï¼Œå¹¶ä¿å­˜åˆ°ç£ç›˜ã€‚
    Hier wird ein FAISS-Index mit den Embeddings aufgebaut und auf die Platte gespeichert.
    """
    num_vectors, dim = embeddings.shape
    print(f"ðŸ“¦ Baue FAISS-Index auf. Anzahl Vektoren: {num_vectors}, Dimension: {dim}")

    # ä½¿ç”¨ Inner Productï¼ˆå†…ç§¯ï¼‰ç´¢å¼•ï¼Œé…åˆå½’ä¸€åŒ–åŽçš„å‘é‡ â‰ˆ ä½™å¼¦ç›¸ä¼¼åº¦
    # Inner Product Index mit normalisierten Vektoren â‰ˆ Kosinus-Ã„hnlichkeit
    index = faiss.IndexFlatIP(dim)

    # å‘é‡å·²ç»åœ¨ encode æ—¶å½’ä¸€åŒ–è¿‡ï¼Œå¯ä»¥ç›´æŽ¥ add
    index.add(embeddings)

    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(out_path))

    print(f"âœ… FAISS-Index gespeichert unter: {out_path}")

# è¿™ä¸ªå‡½æ•°æŠŠ embedding çŸ©é˜µä¸¢è¿› FAISSï¼Œåˆ›å»ºä¸€ä¸ªåŸºäºŽå†…ç§¯çš„ç´¢å¼•ï¼Œå¹¶å­˜æˆ .index æ–‡ä»¶ã€‚
# Die Funktion erstellt mit FAISS einen Inner-Product-Index aus der Embedding-Matrix
# und speichert ihn als .index-Datei.


def main():
    # 1. è¯»å– besser Chunk / bereinigte Chunks laden
    chunks = load_chunks(CHUNK_INPUT_PATH)
    print(f"ðŸ“¥ Geladene Chunks: {len(chunks)}")

    if not chunks:
        print("âŒ Keine Chunks geladen. Bitte Pfad prÃ¼fen.")
        return

    # 2. è®¡ç®—æ‰€æœ‰ chunks çš„ embeddings / Embeddings berechnen
    embeddings = encode_chunks(chunks, MODEL_NAME)
    print(f"ðŸ§© Embeddings-Form: {embeddings.shape}")  # (N, D)

    # 3. ä¿å­˜ embedding çŸ©é˜µ & mapping / Embeddings & Mapping speichern
    save_embeddings(embeddings, EMB_OUTPUT_PATH)
    save_mapping(chunks, MAPPING_OUTPUT_PATH)
    print(f"ðŸ’¾ Embeddings gespeichert: {EMB_OUTPUT_PATH}")
    print(f"ðŸ’¾ Mapping gespeichert:    {MAPPING_OUTPUT_PATH}")

    # 4. æž„å»ºå¹¶ä¿å­˜ FAISS ç´¢å¼• / FAISS-Index aufbauen und speichern
    build_faiss_index(embeddings, FAISS_INDEX_PATH)

    print("\nðŸŽ‰ Schritt 3 abgeschlossen!")
    print("   -> Chunks wurden vektorisiert und in einem FAISS-Index gespeichert.")


if __name__ == "__main__":
    main()
