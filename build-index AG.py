from pathlib import Path
import json
from typing import List, Dict

import numpy as np
from sentence_transformers import SentenceTransformer
import faiss


# ===== è·¯å¾„é…ç½® =====
CHUNK_INPUT_PATH = r"D:\KI-Agent\out\AG-besser chunk.jsonl"

EMB_OUTPUT_PATH = r"D:\KI-Agent\out\AG_besser_embeddings.npy"
MAPPING_OUTPUT_PATH = r"D:\KI-Agent\out\AG_besser_mapping.jsonl"
FAISS_INDEX_PATH = r"D:\KI-Agent\out\AG_besser_faiss.index"

MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"


def load_chunks(path: str) -> List[Dict]:
    """ä» JSONL æ–‡ä»¶è¯»å–æ‰€æœ‰ chunksï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰"""
    chunks: List[Dict] = []
    with open(path, "r", encoding="utf-8") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                chunks.append(json.loads(line))
            except json.JSONDecodeError as e:
                raise ValueError(f"JSON è§£æå¤±è´¥ï¼š{path} ç¬¬ {line_no} è¡Œï¼š{e}") from e
    return chunks


def _get_text(ch: Dict) -> str:
    """å…¼å®¹ä¸åŒ chunk å­—æ®µåï¼štext / chunk / content"""
    return (ch.get("text") or ch.get("chunk") or ch.get("content") or "").strip()


def encode_chunks(chunks: List[Dict], model_name: str) -> np.ndarray:
    """ç”¨ SentenceTransformer å¯¹æ¯ä¸ª chunk æ–‡æœ¬ç®—å‘é‡"""
    model = SentenceTransformer(model_name)
    texts = [_get_text(ch) for ch in chunks]

    print(f"ğŸ§  è®¡ç®— embeddings...ï¼ˆtexts: {len(texts)}ï¼‰")
    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True,  # å½’ä¸€åŒ–åç”¨ Inner Product â‰ˆ Cosine
    )
    if embeddings.dtype != np.float32:
        embeddings = embeddings.astype(np.float32)
    return embeddings


def save_embeddings(embeddings: np.ndarray, path: str) -> None:
    """ä¿å­˜ embedding çŸ©é˜µä¸º .npy"""
    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    np.save(out_path, embeddings)


def save_mapping(chunks: List[Dict], path: str) -> None:
    """
    ä¿å­˜ index -> chunk çš„æ˜ å°„è¡¨ï¼ˆJSONLï¼‰
    é»˜è®¤å†™ idx, id, page, section, docï¼ˆå­—æ®µä¸å­˜åœ¨ä¹Ÿæ²¡äº‹ï¼Œä¿ç•™ä¸º nullï¼‰
    """
    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("w", encoding="utf-8") as f:
        for idx, ch in enumerate(chunks):
            rec = {
                "idx": idx,
                "id": ch.get("id"),
                "page": ch.get("page"),
                "section": ch.get("section"),
                "doc": ch.get("doc"),
            }
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")


def build_faiss_index(embeddings: np.ndarray, path: str) -> None:
    """å»ºç«‹ FAISS ç´¢å¼•å¹¶ä¿å­˜"""
    num_vectors, dim = embeddings.shape
    print(f"ğŸ“¦ å»º FAISS index...ï¼ˆvectors: {num_vectors}, dim: {dim}ï¼‰")

    index = faiss.IndexFlatIP(dim)  # Inner Product + normalize => cosine
    index.add(embeddings)

    out_path = Path(path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    faiss.write_index(index, str(out_path))

    print(f"âœ… FAISS index å·²ä¿å­˜ï¼š{out_path}")


def main() -> None:
    chunks = load_chunks(CHUNK_INPUT_PATH)
    print(f"ğŸ“¥ chunks è¯»å–å®Œæˆï¼š{len(chunks)}")

    if not chunks:
        print("âŒ chunks ä¸ºç©ºï¼šæ£€æŸ¥ CHUNK_INPUT_PATH æ˜¯å¦æ­£ç¡®")
        return

    embeddings = encode_chunks(chunks, MODEL_NAME)
    print(f"ğŸ§© embeddings shape: {embeddings.shape}")  # (N, D)

    save_embeddings(embeddings, EMB_OUTPUT_PATH)
    save_mapping(chunks, MAPPING_OUTPUT_PATH)
    print(f"ğŸ’¾ embeddings: {EMB_OUTPUT_PATH}")
    print(f"ğŸ’¾ mapping:    {MAPPING_OUTPUT_PATH}")

    build_faiss_index(embeddings, FAISS_INDEX_PATH)

    print("\nğŸ‰ å®Œäº‹ï¼ä½ ç°åœ¨å¯ä»¥ç›´æ¥ç”¨ faiss.index + mapping å»æ£€ç´¢äº†ã€‚")


if __name__ == "__main__":
    main()
