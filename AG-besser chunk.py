from pathlib import Path
import json
import re
from typing import List, Dict

# ===== è·¯å¾„é…ç½® / Pfad-Konfiguration =====
INPUT_PATH = r"D:\KI-Agent\out\AG chunk.jsonl"          # ç°æœ‰çš„ç²— chunk æ–‡ä»¶
OUTPUT_PATH = r"D:\KI-Agent\out\AG-besser chunk.jsonl"  # è¾“å‡ºæ›´ç»†çš„ chunk æ–‡ä»¶

MAX_CHARS = 350  # æ¯ä¸ª chunk æœ€å¤§å­—ç¬¦æ•°ä¸Šé™ / maximale ZeichenlÃ¤nge pro Chunk


def load_chunks(path: str) -> List[Dict]:
    """
    ä» JSONL æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰ chunkï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰ã€‚
    Liest alle Chunks aus einer JSONL-Datei (eine Zeile = ein JSON-Objekt).
    """
    chunks: List[Dict] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            chunks.append(json.loads(line))
    return chunks

# è¿™ä¸ªå‡½æ•°è´Ÿè´£æŠŠç°æœ‰çš„ AG chunk æ–‡ä»¶å…¨éƒ¨è¯»è¿›æ¥ï¼Œå˜æˆ Python å­—å…¸åˆ—è¡¨ã€‚
# Diese Funktion lÃ¤dt die bestehende AG-Chunk-Datei und gibt eine Liste von Dicts zurÃ¼ck.


def preprocess_text(text: str) -> str:
    """
    åœ¨åˆ†å¥å‰åšä¸€ç‚¹é¢„å¤„ç†ï¼š
    - åœ¨ (1) (2) (3)... è¿™ç§ç¼–å·å‰é¢å¼ºè¡Œæ–­å¼€ä¸€è¡Œï¼Œè®©å®ƒä»¬å•ç‹¬æˆä¸€å¥çš„å¼€å¤´ã€‚

    Kleine Vorverarbeitung vor dem Satz-Splitting:
    - Vor Nummerierungen wie (1) (2) (3) wird ein Zeilenumbruch eingefÃ¼gt,
      damit sie einen eigenen Satzanfang bilden.
    """
    text = re.sub(r'\s*(\(\d+\))', r'\n\1', text)
    return text

# è¿™ä¸ªå‡½æ•°çš„ç›®çš„æ˜¯æŠŠæ¡æ¬¾ç¼–å· (1)(2)(3)... ä»ä¸Šä¸€å¥æ‹†å¼€ï¼Œé¿å…é»åœ¨å‰ä¸€å¥åé¢ã€‚
# Ziel dieser Funktion ist es, Nummerierungen (1)(2)(3)... vom vorherigen Satz zu trennen.


def split_into_sentences(text: str) -> List[str]:
    """
    æŠŠæ–‡æœ¬æ‹†æˆå¥å­åˆ—è¡¨ï¼š
    - å…ˆé¢„å¤„ç†ç¼–å· (1)(2)...
    - å†æŒ‰æ¢è¡Œå’Œ . ? ! åçš„ç©ºæ ¼åˆ†å¥ã€‚

    Teilt den Text in eine Satzliste:
    - Vorverarbeitung der Nummerierungen (1)(2)...
    - Danach Split anhand von ZeilenumbrÃ¼chen und . ? ! gefolgt von Leerzeichen.
    """
    text = preprocess_text(text)

    rough_parts = re.split(r'\n+', text)
    sentences: List[str] = []

    for part in rough_parts:
        part = part.strip()
        if not part:
            continue
        sub = re.split(r'(?<=[\.\?\!])\s+', part)
        for s in sub:
            s = s.strip()
            if s:
                sentences.append(s)

    return sentences

# è¿™é‡Œå®ç°äº†æŒ‰æ ‡ç‚¹åˆ‡å¥çš„é€»è¾‘ï¼šå…ˆæŒ‰æ¢è¡Œç²—åˆ†ï¼Œå†æŒ‰ . ? ! åˆ†æˆçŸ­å¥ã€‚
# Hier wird ein einfaches Satz-Splitting umgesetzt.


def split_long_sentence_by_length(sentence: str, max_chars: int = MAX_CHARS) -> List[str]:
    """
    å¦‚æœæŸä¸ªå¥å­å¤ªé•¿ï¼ˆ> max_charsï¼‰ï¼Œå†æŒ‰é•¿åº¦åˆ‡ä¸€åˆ€ï¼š
    - å°½é‡åœ¨ç©ºæ ¼å¤„åˆ†æ®µï¼›
    - å®åœ¨æ²¡æœ‰ç©ºæ ¼ï¼Œå°±ç¡¬åˆ‡ã€‚

    Wenn ein Satz zu lang ist (> max_chars), wird er weiter aufgeteilt:
    - bevorzugt an Leerzeichen;
    - notfalls harter Cut.
    """
    s = sentence.strip()
    if len(s) <= max_chars:
        return [s]

    parts: List[str] = []
    start = 0
    L = len(s)
    while start < L:
        end = min(start + max_chars, L)
        # åœ¨ start~end èŒƒå›´å†…æ‰¾æœ€åä¸€ä¸ªç©ºæ ¼
        split_pos = s.rfind(" ", start, end)
        if split_pos == -1 or split_pos <= start:
            split_pos = end
        part = s[start:split_pos].strip()
        if part:
            parts.append(part)
        start = split_pos
    return parts

# è¿™ä¸ªå‡½æ•°æ˜¯ç»™â€œè¶…é•¿å¥å­â€ç”¨çš„äºŒæ¬¡åˆ‡å‰²ï¼Œä¿è¯æ¯ä¸€å°å—ä¸ä¼šè¶…è¿‡ MAX_CHARS å­—ç¬¦ã€‚
# Diese Funktion dient dazu, sehr lange SÃ¤tze weiter zu zerlegen.


def refine_chunk_text(text: str) -> List[str]:
    """
    å¯¹ä¸€ä¸ªåŸå§‹ chunk çš„ text åšäºŒæ¬¡ç»†åˆ‡ï¼š
    1. å…ˆæŒ‰å¥å­æ‹†åˆ†ï¼›
    2. å¯¹æ¯ä¸ªå¥å­ï¼Œå¦‚æœå¤ªé•¿ï¼Œå†æŒ‰æœ€å¤§é•¿åº¦æ‹†åˆ†ã€‚

    Verfeinert den Text eines ursprÃ¼nglichen Chunks:
    1. in SÃ¤tze teilen;
    2. zu lange SÃ¤tze weiter nach max_chars aufsplitten.
    """
    sentences = split_into_sentences(text)
    refined: List[str] = []
    for sent in sentences:
        small_parts = split_long_sentence_by_length(sent, max_chars=MAX_CHARS)
        refined.extend(small_parts)
    return refined

# è¿™ä¸ªå‡½æ•°å®ç°äº†â€œå†åˆ‡ä¸€åˆ€â€çš„æ ¸å¿ƒé€»è¾‘ï¼šå…ˆæŒ‰å¥å·åˆ‡æˆå¥å­ï¼Œå†æŠŠè¶…é•¿å¥å­æŒ‰é•¿åº¦åˆ‡ã€‚
# Diese Funktion bildet das HerzstÃ¼ck der Verfeinerung.


def refine_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    æŠŠåŸæ¥çš„ chunk åˆ—è¡¨å˜æˆæ›´ç»†çš„ chunk åˆ—è¡¨ï¼š
    - æ¯ä¸ªæ—§ chunk å¯èƒ½æ‹†æˆå¤šä¸ªæ–°çš„ chunkï¼›
    - å…ƒä¿¡æ¯ doc/page/section/type ç»§æ‰¿ï¼›
    - id åœ¨åŸ id åŸºç¡€ä¸ŠåŠ  _1/_2/...ã€‚

    Wandelt die ursprÃ¼ngliche Chunk-Liste in eine feinere Liste um:
    - Ein alter Chunk kann in mehrere neue aufgeteilt werden;
    - Metadaten (doc/page/section/type) werden Ã¼bernommen;
    - id bekommt Suffixe _1/_2/... je nach TeilstÃ¼ck.
    """
    new_chunks: List[Dict] = []

    for ch in chunks:
        base_id = ch.get("id", "chunk")
        doc = ch.get("doc")
        page = ch.get("page")
        section = ch.get("section")
        ctype = ch.get("type", "paragraph")
        text = ch.get("text", "")

        parts = refine_chunk_text(text)
        if len(parts) == 0:
            continue

        # å¦‚æœåªåˆ‡æˆ 1 æ®µï¼Œå¯ä»¥ä¿æŒåŸ idï¼›å¦‚æœå¤šæ®µå°±åŠ åç¼€
        if len(parts) == 1:
            new_chunks.append({
                "id": base_id,
                "doc": doc,
                "page": page,
                "type": ctype,
                "section": section,
                "text": parts[0],
            })
        else:
            for i, t in enumerate(parts, start=1):
                new_id = f"{base_id}_{i:02d}"
                new_chunks.append({
                    "id": new_id,
                    "doc": doc,
                    "page": page,
                    "type": ctype,
                    "section": section,
                    "text": t,
                })

    return new_chunks

# è¿™ä¸ªå‡½æ•°éå†æ‰€æœ‰æ—§ chunkï¼Œå¯¹æ¯ä¸€ä¸ªåš refine_chunk_textï¼Œ
# ç„¶åç”Ÿæˆå¸¦æ–° id çš„å° chunkã€‚
# Diese Funktion iteriert Ã¼ber alle alten Chunks und erzeugt daraus
# verfeinerte Chunks mit neuen IDs.


def main():
    # 0. æ£€æŸ¥è¾“å…¥æ–‡ä»¶ / Eingabedatei prÃ¼fen
    in_path = Path(INPUT_PATH)
    if not in_path.exists():
        print(f"âŒ Eingabedatei existiert nicht: {in_path}")
        return
    else:
        print(f"ğŸ“„ Eingabedatei gefunden: {in_path} (GrÃ¶ÃŸe: {in_path.stat().st_size} Bytes)")

    # 1. è¯»å–åŸå§‹ chunks / ursprÃ¼ngliche Chunks laden
    chunks = load_chunks(INPUT_PATH)
    orig_count = len(chunks)
    print(f"ğŸ“¥ Geladene Chunks (original): {orig_count}")

    if not chunks:
        print("âŒ Keine Chunks geladen. Bitte Dateiinhalt prÃ¼fen.")
        return

    # 2. åšäºŒæ¬¡ç»†åˆ‡ / Chunks verfeinern
    better = refine_chunks(chunks)
    final_count = len(better)
    print(f"âœ¨ Nach Verfeinerung: {final_count} Chunks")

    # 3. å†™å‡ºåˆ°æ–°çš„ JSONL / in neue JSONL-Datei schreiben
    out_path = Path(OUTPUT_PATH)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        for ch in better:
            f.write(json.dumps(ch, ensure_ascii=False))
            f.write("\n")

    print("\nâœ… Fertig!")
    print(f"   Ausgabedatei: {out_path}")
    print(f"\nğŸ“Š ç»Ÿè®¡ / Statistik:")
    print(f"   åŸå§‹ Chunk æ•°é‡ (original): {orig_count}")
    print(f"   ç»†åˆ‡å Chunk æ•°é‡ (feiner): {final_count}")

    # 4. çœ‹å‡ ä¸ªä¾‹å­ / ein paar Beispiele anzeigen
    print("\nğŸ‘€ Beispiel-Chunks (erste 10):\n")
    for ch in better[:10]:
        print("----")
        print("id:      ", ch.get("id"))
        print("Seite:   ", ch.get("page"))
        print("Abschnitt:", ch.get("section"))
        print("LÃ¤nge:   ", len(ch.get("text", "")))
        print("Text:    ", ch.get("text", "")[:200], "...")
        print()


if __name__ == "__main__":
    main()
