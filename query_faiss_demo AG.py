from pathlib import Path
import json
from typing import List, Dict

import faiss
from sentence_transformers import SentenceTransformer

from openai import OpenAI
from dotenv import load_dotenv
import os


# ===== ç¯å¢ƒå˜é‡ & OpenAI å®¢æˆ·ç«¯ =====
load_dotenv()  # ä» .env è¯»å– OPENAI_API_KEY
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ===== è·¯å¾„é…ç½® =====
BASE_DIR = Path(r"D:\KI-Agent")

# ä½ çš„ chunk æ–‡ä»¶ï¼ˆæ³¨æ„ï¼šFAISS ç´¢å¼•æ˜¯ç”¨å“ªä¸ª chunks å»ºçš„ï¼Œè¿™é‡Œå°±å¿…é¡»åŠ è½½åŒä¸€ä»½ï¼‰
CHUNK_INPUT_PATH = BASE_DIR / "out" / "AG-besser chunk.jsonl"

# ä½ çš„ FAISS ç´¢å¼•æ–‡ä»¶ï¼ˆä½ åˆšå»ºç«‹å‡ºæ¥çš„é‚£ä¸ªï¼‰
FAISS_INDEX_PATH = BASE_DIR / "out" / "AG_besser_faiss.index"

MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# å¿…é¡»ä¸æ„å»º FAISS ç´¢å¼•æ—¶ä½¿ç”¨çš„ embedding æ¨¡å‹ä¸€è‡´


# ===== è¾“å‡ºè¯­è¨€è§„åˆ™ =====
LANG_RULES = {
    "de": "Antworte ausschlieÃŸlich auf Deutsch.",
    "zh": "åªç”¨ä¸­æ–‡å›ç­”ï¼ˆæ¨¡å—/è¯¾ç¨‹/æ¡ä¾‹åç§°ä¿ç•™å¾·è¯­åŸåï¼‰ã€‚",
    "en": "Answer only in English (keep German names unchanged).",
    "all": "Gib die Antwort dreisprachig in genau dieser Reihenfolge aus: [DE], [ä¸­æ–‡], [EN]."
}

NO_CONTEXT_MSG = {
    "de": "In den bereitgestellten Unterlagen wurden keine passenden Informationen fÃ¼r diese Frage gefunden.",
    "zh": "åœ¨æä¾›çš„èµ„æ–™ä¸­æ²¡æœ‰æ‰¾åˆ°èƒ½ç›´æ¥å›ç­”è¿™ä¸ªé—®é¢˜çš„å†…å®¹ã€‚",
    "en": "No relevant information was found in the provided materials for this question.",
    "all": "[DE]\nIn den bereitgestellten Unterlagen wurden keine passenden Informationen fÃ¼r diese Frage gefunden.\n\n"
           "[ä¸­æ–‡]\nåœ¨æä¾›çš„èµ„æ–™ä¸­æ²¡æœ‰æ‰¾åˆ°èƒ½ç›´æ¥å›ç­”è¿™ä¸ªé—®é¢˜çš„å†…å®¹ã€‚\n\n"
           "[EN]\nNo relevant information was found in the provided materials for this question."
}


def load_chunks(path: Path) -> List[Dict]:
    """è¯»å– chunk jsonlï¼ˆæ¯è¡Œä¸€ä¸ª jsonï¼‰"""
    chunks: List[Dict] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            chunks.append(json.loads(line))
    return chunks


def load_faiss_index(path: Path) -> faiss.Index:
    """è¯»å– FAISS index"""
    return faiss.read_index(str(path))


def load_model(model_name: str) -> SentenceTransformer:
    """åŠ è½½ embedding æ¨¡å‹ï¼ˆè¦å’Œå»ºç´¢å¼•æ—¶åŒä¸€ä¸ªï¼‰"""
    return SentenceTransformer(model_name)


def search(
    query: str,
    model: SentenceTransformer,
    index: faiss.Index,
    chunks: List[Dict],
    top_k: int = 5,
):
    """
    - encode query
    - FAISS top_k
    - ç”¨ idx å›è¡¨ chunks
    """
    query_emb = model.encode(
        [query],
        normalize_embeddings=True,
        convert_to_numpy=True,
    )

    D, I = index.search(query_emb, top_k)

    results = []
    for rank, idx in enumerate(I[0]):
        if idx == -1:
            continue
        if idx >= len(chunks):
            continue  # é˜²æ­¢ index/chunks ä¸å¯¹é½ç›´æ¥ç‚¸
        ch = chunks[idx]
        results.append(
            {
                "rank": rank + 1,
                "score": float(D[0][rank]),
                "page": ch.get("page"),
                "section": ch.get("section"),
                "doc": ch.get("doc"),
                "text": ch.get("text", ""),
            }
        )
    return results


def build_context(results: List[Dict], max_chars: int = 3000) -> str:
    """æŠŠæ£€ç´¢åˆ°çš„ chunk æ‹¼æˆ contextï¼ˆå¸¦ doc/page/section æ–¹ä¾¿å¼•ç”¨ï¼‰"""
    parts: List[str] = []
    length = 0

    for item in results:
        text = (item.get("text") or "").strip()
        if not text:
            continue

        header = f"[Dokument: {item.get('doc')} | Seite: {item.get('page')} | Abschnitt: {item.get('section')}]"
        block = f"{header}\n{text}"

        if length + len(block) > max_chars:
            break

        parts.append(block)
        length += len(block)

    return "\n\n".join(parts)


def translate_query_to_german(query: str) -> str:
    """
    ï¼ˆå¯é€‰ï¼‰æŠŠç”¨æˆ·é—®é¢˜ç¿»æˆå¾·è¯­ç”¨äºæ£€ç´¢ï¼Œæé«˜å¬å›
    åªè¿”å›ç¿»è¯‘æ–‡æœ¬
    """
    prompt = (
        "Ãœbersetze die folgende Frage ins Deutsche. "
        "Gib NUR die Ãœbersetzung aus, ohne ErklÃ¤rungen:\n\n"
        f"{query}"
    )
    resp = client.responses.create(
        model="gpt-4o-mini",
        input=prompt,
        max_output_tokens=120,
    )
    return (resp.output_text or "").strip()


def generate_answer_with_llm(question: str, context: str, output_lang: str) -> str:
    """OpenAI LLMï¼šåªå…è®¸åŸºäº context ä½œç­”ï¼Œæ²¡è¯æ®å°±ç›´è¯´"""
    output_lang = (output_lang or "de").lower()
    if output_lang not in LANG_RULES:
        output_lang = "de"

    if not context.strip():
        return NO_CONTEXT_MSG[output_lang]

    lang_rule = LANG_RULES[output_lang]

    prompt = f"""
Du bist ein Studienassistent. Du darfst NUR auf Basis des gegebenen Kontexts antworten.
Wenn der Kontext keine Antwort enthÃ¤lt, sage das klar (keine Erfindungen).

Wichtig (Ausgabe-Regeln):
- {lang_rule}
- Behalte Modul-/Kursnamen im Original (Deutsch).
- Antworte klar, prÃ¤zise; wenn sinnvoll mit Stichpunkten.
- Wenn du dir unsicher bist, sag es.

[Kontext]
{context}

[Frage]
{question}
""".strip()

    response = client.responses.create(
        model="gpt-4o-mini",
        input=prompt,
        max_output_tokens=512,
    )

    return (response.output_text or "").strip()


def answer_question(
    query: str,
    model: SentenceTransformer,
    index: faiss.Index,
    chunks: List[Dict],
    output_lang: str = "de",
    translate_for_retrieval: bool = False,
) -> Dict:
    """
    - (å¯é€‰) å…ˆç¿»è¯‘ä¸ºå¾·è¯­ç”¨äºæ£€ç´¢
    - search() æ‰¾ chunks
    - build_context()
    - LLM è¾“å‡º
    """
    retrieval_query = query
    if translate_for_retrieval:
        try:
            retrieval_query = translate_query_to_german(query)
        except Exception:
            retrieval_query = query

    hits = search(retrieval_query, model, index, chunks, top_k=5)
    context = build_context(hits, max_chars=3000)
    answer_text = generate_answer_with_llm(query, context, output_lang)

    return {
        "answer": answer_text,
        "hits": hits,
        "retrieval_query": retrieval_query,
    }


def main():
    print("ğŸ“¥ åŠ è½½æ¨¡å‹ / index / chunks ...")

    if not CHUNK_INPUT_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° chunksï¼š{CHUNK_INPUT_PATH}")
    if not FAISS_INDEX_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° faiss indexï¼š{FAISS_INDEX_PATH}")

    chunks = load_chunks(CHUNK_INPUT_PATH)
    index = load_faiss_index(FAISS_INDEX_PATH)
    model = load_model(MODEL_NAME)

    print(f"   âœ chunks: {len(chunks)}")
    print(f"   âœ index.ntotal: {index.ntotal}")

    # å¯¹é½æ£€æŸ¥ï¼šå¦‚æœä¸ä¸€è‡´ï¼Œæ£€ç´¢ä¼šé”™ä½ï¼ˆå¼ºæé†’ï¼‰
    if index.ntotal != len(chunks):
        print("\nâš ï¸ è­¦å‘Šï¼šindex å‘é‡æ•° å’Œ chunks è¡Œæ•°ä¸ä¸€è‡´ï¼")
        print("   è¿™ä¼šå¯¼è‡´æ£€ç´¢ idx å›è¡¨é”™ä½ã€‚")
        print("   è§£å†³åŠæ³•ï¼šç”¨â€œå»ºç«‹ index æ—¶ä½¿ç”¨çš„é‚£ä»½ chunks æ–‡ä»¶â€æ¥æ£€ç´¢ï¼ˆå¿…é¡»ä¸€è‡´ï¼‰ã€‚\n")

    print("âœ… Die Initialisierung ist abgeschlossen, du kannst jetzt Fragen stellen!\n")

    mode = input("æ¨¡å¼ï¼š1=åªæ£€ç´¢ 2=æ£€ç´¢+LLMï¼ˆé»˜è®¤ 2ï¼‰ï¼š").strip() or "2"

    output_lang = input("Wahlen Spracheï¼ˆde/zh/en/allï¼‰[de]ï¼š").strip().lower() or "de"
    if output_lang not in LANG_RULES:
        output_lang = "de"

    translate_for_retrieval = False  # é»˜è®¤å…³é—­ï¼š/retrieval on å¼€å¯

    print("\nğŸ’¡ å¿«æ·æŒ‡ä»¤ï¼š")
    print("   /lang de|zh|en|all    â†’ åˆ‡æ¢è¾“å‡ºè¯­è¨€")
    print("   /retrieval on|off     â†’ æ˜¯å¦å…ˆç¿»æˆå¾·è¯­å†æ£€ç´¢ï¼ˆæå‡å¬å›ï¼‰")
    print("   /help                 â†’ æŸ¥çœ‹æŒ‡ä»¤")
    print("   q                      â†’ é€€å‡º\n")

    while True:
        query = input("Bitte Fragen").strip()
        if not query:
            continue
        if query.lower() == "q":
            print("ğŸ‘‹ ENd")
            break

        if query.startswith("/help"):
            print("\nCommands:")
            print("  /lang de|zh|en|all")
            print("  /retrieval on|off")
            print("  q (quit)\n")
            continue

        if query.startswith("/lang"):
            parts = query.split()
            if len(parts) >= 2 and parts[1].lower() in LANG_RULES:
                output_lang = parts[1].lower()
                print(f"âœ… è¾“å‡ºè¯­è¨€åˆ‡æ¢ä¸º: {output_lang}\n")
            else:
                print("âŒ ç”¨æ³•ï¼š/lang de|zh|en|all\n")
            continue

        if query.startswith("/retrieval"):
            parts = query.split()
            if len(parts) >= 2 and parts[1].lower() in ["on", "off"]:
                translate_for_retrieval = (parts[1].lower() == "on")
                state = "ON" if translate_for_retrieval else "OFF"
                print(f"âœ… retrieval ç¿»è¯‘æ¨¡å¼: {state}\n")
            else:
                print("âŒ ç”¨æ³•ï¼š/retrieval on|off\n")
            continue

        print("\n" + "=" * 80)

        if mode == "2":
            result = answer_question(
                query=query,
                model=model,
                index=index,
                chunks=chunks,
                output_lang=output_lang,
                translate_for_retrieval=translate_for_retrieval,
            )

            print("ã€LLM å›ç­”ã€‘")
            print(result["answer"])

            if translate_for_retrieval:
                print("\n[æ£€ç´¢ç”¨ Queryï¼ˆå¾·è¯­ï¼‰]")
                print(result["retrieval_query"])

            print("\nã€Evidenzpassageã€‘")
            print("-" * 80)
            for h in result["hits"]:
                print(
                    f"[{h['rank']}] score={h['score']:.4f} | doc={h.get('doc')} | page={h.get('page')} | section={h.get('section')}"
                )
                print(h["text"])
                print("-" * 80)
        else:
            hits = search(query, model, index, chunks, top_k=5)
            if not hits:
                print("ğŸ˜¶ æ²¡æœåˆ°ç›¸å…³å†…å®¹ã€‚")
            else:
                for h in hits:
                    print(
                        f"[{h['rank']}] score={h['score']:.4f} | doc={h.get('doc')} | page={h.get('page')} | section={h.get('section')}"
                    )
                    print(h["text"])
                    print("-" * 80)

        print()


if __name__ == "__main__":
    main()
