from pathlib import Path
import json
import re
from typing import List

# ===== è·¯å¾„é…ç½® / Pfad-Konfiguration =====
INPUT_PATH = r"D:\KI-Agent\out\Maschinenbau_und_Mechatronik_clean.json.txt"
OUTPUT_PATH = r"D:\KI-Agent\out\Maschinenbau_und_Mechatronik_chunks.jsonl"
DOC_SHORT_NAME = "MM_SPO13"  # æ–‡æ¡£çŸ­å / Kurzname des Dokuments

# ä¸Šé¢ä¸‰è¡Œæ˜¯ä½ éœ€è¦æ”¹çš„é…ç½®ï¼šè¾“å…¥æ¸…æ´—åæ–‡ä»¶è·¯å¾„ã€è¾“å‡º chunks æ–‡ä»¶è·¯å¾„ã€æ–‡æ¡£ç®€ç§°ã€‚
# Die drei Zeilen oben sind die wichtigsten Einstellungen:
# Pfad zur bereinigten Eingabedatei, Pfad zur Ausgabedatei und Dokument-Kurzname.


def load_pages(path: str):
    """
    è¯»å–æ¸…æ´—åçš„ JSONL æ–‡ä»¶ã€‚
    ä¸€è¡Œä¸€é¡µï¼Œè¿”å›ä¸€ä¸ª dict åˆ—è¡¨ï¼Œæ¯ä¸ª dict ä»£è¡¨ä¸€é¡µã€‚
    éœ€è¦æ–‡ä»¶é‡Œæ¯è¡Œè‡³å°‘æœ‰: page, text è¿™ä¸¤ä¸ªå­—æ®µã€‚

    LÃ¤dt die bereinigte JSONL-Datei.
    Eine Zeile entspricht einer Seite als Dict.
    Jede Zeile braucht mindestens die Felder: page, text.
    """
    pages = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            pages.append(json.loads(line))
    return pages

# è¿™ä¸ªå‡½æ•°è´Ÿè´£æŠŠ JSONL è¯»è¿›æ¥ï¼Œæ¯ä¸€è¡Œï¼ˆä¸€é¡µï¼‰å˜æˆä¸€ä¸ª Python å­—å…¸ã€‚
# Diese Funktion liest die JSONL-Datei ein und wandelt jede Zeile (eine Seite)
# in ein Python-Dict um.


def preprocess_text(text: str) -> str:
    """
    åœ¨åˆ†å¥å‰åšä¸€ç‚¹é¢„å¤„ç†ï¼š
    - åœ¨ (1) (2) (3)... è¿™ç§ç¼–å·å‰é¢å¼ºè¡Œæ–­å¼€ä¸€è¡Œï¼Œè®©å®ƒä»¬å•ç‹¬æˆä¸€å¥çš„å¼€å¤´ã€‚

    Kleine Vorverarbeitung vor dem Satz-Splitting:
    - Vor Nummerierungen wie (1) (2) (3) wird ein Zeilenumbruch eingefÃ¼gt,
      damit sie einen eigenen Satzanfang bilden.
    """
    text = re.sub(r'\s*(\(\d+\))', r'\n\1', text)
    return text

# è¿™ä¸ªå‡½æ•°çš„ç›®çš„æ˜¯æŠŠæ¡æ¬¾ç¼–å· (1)(2)(3)... ä»ä¸Šä¸€å¥æ‹†å¼€ï¼Œé¿å…é»åœ¨å‰ä¸€å¥åé¢ã€‚
# Ziel dieser Funktion ist es, Nummerierungen (1)(2)(3)... vom vorherigen Satz zu trennen,
# damit sie sauber als eigener Satzanfang behandelt werden.


def split_into_sentences(text: str) -> List[str]:
    """
    æŠŠæ–‡æœ¬æ‹†æˆå¥å­åˆ—è¡¨ï¼š
    - å…ˆé¢„å¤„ç†ç¼–å· (1)(2)...
    - å†æŒ‰æ¢è¡Œå’Œ . ? ! åçš„ç©ºæ ¼åˆ†å¥ã€‚

    Teilt den Text in eine Satzliste:
    - Vorverarbeitung der Nummerierungen (1)(2)...
    - Danach Split anhand von ZeilenumbrÃ¼chen und . ? ! gefolgt von Leerzeichen.
    """
    text = preprocess_text(text)

    rough_parts = re.split(r'\n+', text)
    sentences: List[str] = []

    for part in rough_parts:
        part = part.strip()
        if not part:
            continue
        sub = re.split(r'(?<=[\.\?\!])\s+', part)
        for s in sub:
            s = s.strip()
            if s:
                sentences.append(s)

    return sentences

# è¿™é‡Œå®ç°äº†ä¸€ä¸ªç®€å•çš„â€œæŒ‰å¥å­åˆ‡åˆ†â€çš„é€»è¾‘ï¼šå…ˆæŒ‰æ¢è¡Œç²—åˆ†ï¼Œå†æŒ‰ . ? ! åˆ†æˆçŸ­å¥ã€‚
# Hier wird ein einfaches Satz-Splitting umgesetzt: zuerst grob per Zeilenumbruch,
# dann feiner per . ? ! in einzelne SÃ¤tze.


def make_sentence_chunks(
    sentences: List[str],
    max_sent_per_chunk: int = 1,
    overlap: int = 0
) -> List[str]:
    """
    æŒ‰å¥å­ç”Ÿæˆ chunkã€‚
    é»˜è®¤ï¼šmax_sent_per_chunk=1ï¼Œä¸€å¥ä¸€ä¸ª chunkï¼Œä¸é‡å ã€‚

    Erzeugt Chunks aus der Satzliste.
    Standard: max_sent_per_chunk=1, ein Satz pro Chunk, ohne Ãœberlappung.
    """
    chunks = []
    if not sentences:
        return chunks

    step = max_sent_per_chunk - overlap
    if step <= 0:
        raise ValueError("max_sent_per_chunk å¿…é¡»å¤§äº overlap")

    start = 0
    while start < len(sentences):
        end = start + max_sent_per_chunk
        chunk_sents = sentences[start:end]
        if len(chunk_sents) == 0:
            break
        chunk_text = " ".join(chunk_sents)
        chunks.append(chunk_text)
        start += step

    return chunks

# è¿™ä¸ªå‡½æ•°æŠŠå¥å­æ•°ç»„é‡æ–°æ‰“åŒ…æˆ chunkï¼Œç°åœ¨è®¾ç½®ä¸ºâ€œä¸€å¥å°±æ˜¯ä¸€ä¸ª chunkâ€ã€‚
# Diese Funktion packt die Satzliste in Chunks. Momentan wird jeder einzelne Satz
# direkt zu einem Chunk gemacht.


def guess_section(full_text: str, page: int) -> str:
    """
    ç²—ç•¥çŒœæµ‹è¿™ä¸€é¡µå±äºå“ªä¸ª sectionï¼š
    - åŒ…å« Â§ 41 / Tabelle 1 / Tabelle 2 / Tabelle 3 å°±è¿”å›å¯¹åº”æ ‡é¢˜ï¼›
    - å¦åˆ™è¿”å› 'Seite X'ã€‚

    Grobe SchÃ¤tzung, zu welchem Abschnitt die Seite gehÃ¶rt:
    - Wenn der Text Â§ 41 / Tabelle 1 / Tabelle 2 / Tabelle 3 enthÃ¤lt,
      wird der entsprechende Titel genutzt;
    - sonst 'Seite X'.
    """
    if "Â§ 41" in full_text:
        return "Â§ 41 Bachelorstudiengang Maschinenbau und Mechatronik"
    if "Tabelle 1" in full_text:
        return "Tabelle 1 Modulstruktur"
    if "Tabelle 2" in full_text:
        return "Tabelle 2 Grundstudium"
    if "Tabelle 3" in full_text:
        return "Tabelle 3 Hauptstudium"
    return f"Seite {page}"

# è¿™ä¸ªå‡½æ•°åªæ˜¯ç»™æ¯ä¸€é¡µæ‰“ä¸ªå¤§æ¦‚çš„æ ‡ç­¾ï¼šå±äºå“ªä¸ªæ¡æ¬¾/è¡¨æ ¼ï¼Œæˆ–è€…ç›´æ¥â€œç¬¬ X é¡µâ€ã€‚
# Diese Funktion vergibt einen groben Abschnitts-Namen fÃ¼r jede Seite:
# entweder ein Paragraf/Tabelle oder einfach 'Seite X'.


def make_chunks_for_page(
    page_rec: dict,
    doc_short_name: str,
    max_sent_per_chunk: int = 1,
    overlap: int = 0
):
    """
    æŠŠâ€œä¸€é¡µâ€çš„è®°å½•åˆ‡æˆå¤šä¸ª chunk dictï¼ˆé»˜è®¤ä¸€å¥ä¸€ä¸ª chunkï¼‰ã€‚

    Teilt einen Seiten-Datensatz in mehrere Chunk-Dicts
    (Standard: ein Satz pro Chunk).
    """
    page = page_rec.get("page")
    full_text = page_rec.get("text", "")

    sentences = split_into_sentences(full_text)
    chunk_texts = make_sentence_chunks(
        sentences,
        max_sent_per_chunk=max_sent_per_chunk,
        overlap=overlap
    )

    section = guess_section(full_text, page)

    chunks = []
    for i, ctext in enumerate(chunk_texts, start=1):
        chunk_id = f"{doc_short_name}_p{page}_c{i:02d}"
        chunk = {
            "id": chunk_id,
            "doc": doc_short_name,
            "page": page,
            "type": "paragraph",  # ç›®å‰ç»Ÿä¸€æ ‡è®°ä¸º paragraph
            "section": section,
            "text": ctext
        }
        chunks.append(chunk)

    return chunks

# è¿™ä¸ªå‡½æ•°æŠŠâ€œæŸä¸€é¡µâ€çš„ text æ‹†æˆå¾ˆå¤šæ¡ chunkï¼Œæ¯æ¡ chunk å˜æˆä¸€ä¸ªå¸¦å­—æ®µçš„å­—å…¸ã€‚
# Diese Funktion zerlegt den Text einer Seite in viele Chunks und baut
# fÃ¼r jeden Chunk ein Dict mit allen wichtigen Feldern.


def main():
    # 1. è¯»å–æ‰€æœ‰é¡µé¢ / alle Seiten laden
    pages = load_pages(INPUT_PATH)
    if not pages:
        print("âŒ Es wurden keine Seiten geladen. Bitte prÃ¼fe den Pfad in INPUT_PATH.")
        return

    all_chunks = []

    # 2. å¤„ç†æ•´ä¸ªæ–‡æ¡£çš„æ¯ä¸€é¡µ / jede Seite des Dokuments verarbeiten
    for page_rec in pages:
        page = page_rec.get("page")
        print(f"ğŸ”§ Verarbeite Seite {page} ...")
        page_chunks = make_chunks_for_page(
            page_rec,
            doc_short_name=DOC_SHORT_NAME,
            max_sent_per_chunk=1,  # ä¸€å¥ä¸€ä¸ª chunk / ein Satz pro Chunk
            overlap=0
        )
        all_chunks.extend(page_chunks)

    # 3. å†™å‡ºæ‰€æœ‰ chunks åˆ° JSONL / alle Chunks in eine JSONL-Datei schreiben
    out_path = Path(OUTPUT_PATH)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        for ch in all_chunks:
            f.write(json.dumps(ch, ensure_ascii=False))
            f.write("\n")

    print(f"\nâœ… Insgesamt wurden {len(all_chunks)} Chunks erzeugt.")
    print(f"   Ausgabedatei: {out_path}")

    # 4. æ§åˆ¶å°ä¸Šé¡ºä¾¿çœ‹å‡ æ¡ / ein paar Beispiel-Chunks in der Konsole anzeigen
    print("\nğŸ‘€ Beispiel-Chunks:\n")
    for ch in all_chunks[:10]:
        print("----")
        print("id:      ", ch["id"])
        print("Seite:   ", ch["page"])
        print("Abschnitt:", ch["section"])
        print("Text:    ", ch["text"])
        print()

# main() è´Ÿè´£æŠŠæ‰€æœ‰æ­¥éª¤ä¸²èµ·æ¥ï¼šè¯»æ–‡ä»¶ â†’ æ¯é¡µåˆ‡ chunk â†’ å†™å‡º JSONL â†’ æ‰“å°å‰å‡ ä¸ªç¤ºä¾‹ã€‚
# Die main()-Funktion verbindet alle Schritte:
# Datei laden â†’ pro Seite Chunks erzeugen â†’ JSONL schreiben â†’ einige Beispiele ausgeben.


if __name__ == "__main__":
    main()
