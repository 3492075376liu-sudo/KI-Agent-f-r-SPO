from pathlib import Path
import json
from typing import List, Dict

# ===== è·¯å¾„é…ç½® / Pfad-Konfiguration =====
INPUT_PATH = r"D:\KI-Agent\out\Maschinenbau_und_Mechatronik_chunks.jsonl"
OUTPUT_PATH = r"D:\KI-Agent\out\besser Chunk.jsonl"

SHORT_THRESHOLD = 25   # å­—ç¬¦æ•° < è¿™ä¸ªå€¼å°±è®¤ä¸ºå¤ªçŸ­ / Chars < THRESHOLD => "zu kurz"
LONG_THRESHOLD = 500   # å­—ç¬¦æ•° > è¿™ä¸ªå€¼å°±è®¤ä¸ºå¤ªé•¿ / Chars > THRESHOLD => "zu lang"


def load_chunks(path: str) -> List[Dict]:
    """
    ä» JSONL æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰ chunkï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰ã€‚
    Liest alle Chunks aus einer JSONL-Datei (eine Zeile = ein JSON-Objekt).
    """
    chunks: List[Dict] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            chunks.append(json.loads(line))
    return chunks

# è¿™ä¸ªå‡½æ•°è´Ÿè´£æŠŠç°æœ‰çš„ chunk æ–‡ä»¶å…¨éƒ¨è¯»è¿›æ¥ï¼Œå˜æˆ Python å­—å…¸åˆ—è¡¨ã€‚
# Diese Funktion lÃ¤dt die bestehende Chunk-Datei und gibt eine Liste von Dicts zurÃ¼ck.


def merge_short_chunks(chunks: List[Dict],
                       short_threshold: int = SHORT_THRESHOLD) -> List[Dict]:
    """
    åˆå¹¶è¿‡çŸ­çš„ chunkï¼š
    - å¦‚æœä¸€ä¸ª chunk çš„ text å¾ˆçŸ­ï¼ˆ< short_thresholdï¼‰ï¼Œ
      ä¼˜å…ˆæŠŠå®ƒå¹¶åˆ°â€œä¸‹ä¸€ä¸ªåŒé¡µåŒ section çš„ chunkâ€å‰é¢ï¼›
      å¦‚æœæ²¡æœ‰ä¸‹ä¸€ä¸ªï¼Œå°±å¹¶åˆ°å‰ä¸€ä¸ªï¼›
      å®åœ¨éƒ½æ²¡æœ‰ï¼Œå°±ä¿ç•™åŸæ ·ã€‚

    FÃ¼gt zu kurze Chunks zusammen:
    - Wenn der Text eines Chunks sehr kurz ist (< short_threshold),
      wird er bevorzugt an den nÃ¤chsten Chunk (gleiche Seite & section)
      vorn angehÃ¤ngt; falls nicht mÃ¶glich, an den vorherigen.
      Wenn beides nicht geht, bleibt der Chunk unverÃ¤ndert.
    """
    new_chunks: List[Dict] = []
    i = 0
    n = len(chunks)

    while i < n:
        ch = chunks[i]
        text = ch.get("text", "")
        if len(text) < short_threshold:
            # å°è¯•åˆå¹¶åˆ°â€œä¸‹ä¸€ä¸ªâ€ / zuerst mit dem nÃ¤chsten Chunk zusammenschieben
            merged = False
            if i + 1 < n:
                nxt = chunks[i + 1]
                if (nxt.get("page") == ch.get("page")
                        and nxt.get("section") == ch.get("section")):
                    # æŠŠå½“å‰çŸ­æ–‡æœ¬åŠ åˆ°ä¸‹ä¸€æ¡å‰é¢
                    # aktuellen kurzen Text vor den nÃ¤chsten Chunk setzen
                    nxt["text"] = text + " " + nxt.get("text", "")
                    merged = True
                    # å½“å‰ ch ä¸å†™å…¥ new_chunksï¼Œç›´æ¥è·³è¿‡ / ch wird Ã¼bersprungen

            if not merged and new_chunks:
                # åˆå¹¶åˆ°â€œä¸Šä¸€æ¡â€ / an den vorherigen Chunk anhÃ¤ngen
                prev = new_chunks[-1]
                if (prev.get("page") == ch.get("page")
                        and prev.get("section") == ch.get("section")):
                    prev["text"] = prev.get("text", "") + " " + text
                    merged = True

            if not merged:
                # å®åœ¨æ‰¾ä¸åˆ°åˆå¹¶å¯¹è±¡ï¼Œå°±åŸæ ·ä¿ç•™ / falls kein Merge mÃ¶glich ist, Chunk behalten
                new_chunks.append(ch)

            i += 1
        else:
            # æ­£å¸¸é•¿åº¦çš„ chunk ç›´æ¥ä¿ç•™ / normale Chunks einfach Ã¼bernehmen
            new_chunks.append(ch)
            i += 1

    return new_chunks

# è¿™ä¸ªå‡½æ•°ä¼šæŠŠç‰¹åˆ«çŸ­çš„ chunk å°è¯•å’Œå‰åç›¸é‚»çš„ï¼ˆåŒé¡µã€åŒ sectionï¼‰æ‹¼åœ¨ä¸€èµ·ï¼Œ
# é¿å…å‡ºç°åªæœ‰ â€œ- 2.â€ã€â€œ(5) Ende des 4.â€ è¿™ç±»ä¿¡æ¯é‡è¿‡å°çš„å—ã€‚
# Diese Funktion versucht, extrem kurze Chunks mit ihren Nachbarn
# (gleiche Seite & section) zusammenzufÃ¼hren, damit keine Mini-Chunks
# wie â€- 2.â€œ oder â€(5) Ende des 4.â€œ alleine stehen bleiben.


def split_long_chunk(chunk: Dict,
                     long_threshold: int = LONG_THRESHOLD) -> List[Dict]:
    """
    æŠŠä¸€ä¸ªè¿‡é•¿çš„ chunk æŒ‰é•¿åº¦æ‹†æˆå¤šä¸ªå°å—ï¼š
    - ä»¥ long_threshold ä¸ºä¸Šé™ï¼Œå°½é‡åœ¨ç©ºæ ¼å¤„åˆ‡åˆ†ï¼›
    - æ¯ä¸€æ®µä¿ç•™åŸ chunk çš„å…ƒä¿¡æ¯ï¼Œåªæ˜¯ text ç¼©çŸ­ï¼Œid åé¢åŠ  _1, _2, _3...

    Teilt einen zu langen Chunk in mehrere kleinere:
    - nutzt long_threshold als maximale LÃ¤nge und splittet mÃ¶glichst an Leerzeichen;
    - Metadaten bleiben erhalten, nur der Text wird aufgeteilt,
      die id bekommt Suffixe _1, _2, _3...
    """
    text = chunk.get("text", "").strip()
    if len(text) <= long_threshold:
        return [chunk]

    parts: List[Dict] = []
    base_id = chunk.get("id", "chunk")

    start = 0
    part_idx = 1
    L = len(text)

    while start < L:
        # å…ˆå–ä¸€ä¸ªæœ€å¤§çª—å£ / zunÃ¤chst ein Fenster in MaximalgrÃ¶ÃŸe
        end = min(start + long_threshold, L)

        # å°è¯•åœ¨çª—å£ä¸­é—´åˆ° end èŒƒå›´å†…æ‰¾æœ€åä¸€ä¸ªç©ºæ ¼ä½œä¸ºåˆ‡åˆ†ç‚¹
        # Versuche, zwischen Mitte des Fensters und end das letzte Leerzeichen zu finden
        mid = start + long_threshold // 2
        split_pos = text.rfind(" ", mid, end)

        if split_pos == -1 or split_pos <= start:
            # æ‰¾ä¸åˆ°åˆé€‚ç©ºæ ¼ï¼Œå°±ç¡¬åˆ‡ / wenn kein Leerzeichen gefunden, hart splitten
            split_pos = end

        part_text = text[start:split_pos].strip()
        if part_text:
            new_chunk = dict(chunk)  # æµ…æ‹·è´ / Shallow Copy
            new_chunk["id"] = f"{base_id}_{part_idx}"
            new_chunk["text"] = part_text
            parts.append(new_chunk)
            part_idx += 1

        start = split_pos

    return parts

# è¿™ä¸ªå‡½æ•°ä¸“é—¨å¤„ç†ç‰¹åˆ«é•¿çš„ chunkï¼Œä¼šæŒ‰å¤§çº¦ long_threshold çš„é•¿åº¦åˆ‡å—ï¼Œ
# ä¼˜å…ˆåœ¨ç©ºæ ¼å¤„åˆ†æ®µï¼Œåˆ‡å‡ºæ¥çš„æ¯ä¸€æ®µéƒ½ç»§æ‰¿åŸæ¥çš„å…ƒä¿¡æ¯ï¼ˆpage/section ç­‰ï¼‰ã€‚
# Diese Funktion behandelt sehr lange Chunks, indem sie den Text in StÃ¼cke
# von ungefÃ¤hr long_threshold Zeichen aufteilt, mÃ¶glichst an Leerzeichen.
# Jede TeilstÃ¼ck behÃ¤lt die ursprÃ¼nglichen Metadaten wie page/section.


def split_long_chunks(chunks: List[Dict],
                      long_threshold: int = LONG_THRESHOLD) -> List[Dict]:
    """
    å¯¹åˆ—è¡¨é‡Œçš„æ¯ä¸€ä¸ª chunk è¿›è¡Œâ€œå¦‚æœå¤ªé•¿å°±æ‹†åˆ†â€çš„å¤„ç†ã€‚

    Wendet die Split-Logik auf alle Chunks an:
    - wenn ein Chunk zu lang ist, wird er in mehrere Teile zerlegt.
    """
    new_chunks: List[Dict] = []
    for ch in chunks:
        text = ch.get("text", "")
        if len(text) > long_threshold:
            parts = split_long_chunk(ch, long_threshold=long_threshold)
            new_chunks.extend(parts)
        else:
            new_chunks.append(ch)
    return new_chunks

# è¿™ä¸ªå‡½æ•°ä¼šéå†æ‰€æœ‰ chunkï¼Œå¯¹é•¿åº¦è¶…è¿‡ long_threshold çš„è°ƒç”¨ split_long_chunk åšæ‹†åˆ†ã€‚
# Diese Funktion iteriert Ã¼ber alle Chunks und ruft fÃ¼r zu lange Chunks
# split_long_chunk auf, um sie aufzuteilen.


def main():
    # 1. è¯»å–åŸå§‹ chunk æ–‡ä»¶ / Original-Chunk-Datei laden
    chunks = load_chunks(INPUT_PATH)
    orig_count = len(chunks)
    print(f"ğŸ“¥ Geladene Chunks (Original): {orig_count}")

    # 2. åˆå¹¶è¿‡çŸ­çš„ chunks / zu kurze Chunks zusammenfÃ¼hren
    merged = merge_short_chunks(chunks, short_threshold=SHORT_THRESHOLD)
    merged_count = len(merged)
    print(f"ğŸ©¹ Nach Merge der kurzen Chunks: {merged_count} Chunks")

    # 3. æ‹†åˆ†è¿‡é•¿çš„ chunks / zu lange Chunks aufteilen
    better = split_long_chunks(merged, long_threshold=LONG_THRESHOLD)
    final_count = len(better)
    print(f"âœ‚ï¸  Nach Split der langen Chunks: {final_count} Chunks")

    # 4. å†™å‡ºåˆ°æ–°çš„ JSONL æ–‡ä»¶ / in neue JSONL-Datei schreiben
    out_path = Path(OUTPUT_PATH)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        for ch in better:
            f.write(json.dumps(ch, ensure_ascii=False))
            f.write("\n")

    print("\nâœ… Fertig!")
    print(f"   Ausgabedatei: {out_path}")
    print(f"\nğŸ“Š ç»Ÿè®¡ / Statistik:")
    print(f"   åŸå§‹ Chunk æ•°é‡ (Original): {orig_count}")
    print(f"   åˆå¹¶çŸ­ Chunk ä¹‹å (nach Merge): {merged_count}")
    print(f"   æœ€ç»ˆè¾“å‡º Chunk æ•°é‡ (final): {final_count}")

    # 5. éšä¾¿çœ‹å‡ æ¡ç»“æœ / ein paar Beispiel-Chunks anzeigen
    print("\nğŸ‘€ Beispiel-Chunks (erste 10):\n")
    for ch in better[:10]:
        print("----")
        print("id:      ", ch.get("id"))
        print("Seite:   ", ch.get("page"))
        print("Abschnitt:", ch.get("section"))
        print("LÃ¤nge:   ", len(ch.get("text", "")))
        print("Text:    ", ch.get("text", "")[:200], "...")
        print()


if __name__ == "__main__":
    main()
